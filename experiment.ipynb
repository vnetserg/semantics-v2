{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, itertools, sys\n",
    "import pandas as ps\n",
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = ps.read_csv(\"data/spelled-f.csv\", sep=';', header=None,\n",
    "                   index_col=0,names=['id','title','text','cluster','date','publisher'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число записей в таблице: 32317\n"
     ]
    }
   ],
   "source": [
    "data = data[~data[\"cluster\"].isin([\"-\", \"S\", \"Standard \"])]\n",
    "print(\"Число записей в таблице:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведение к нормальной форме и определение граммемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MYSTEM_GRAMMEMS = [\"NAME\", \"A\", \"ADV\", \"ADVPRO\", \"ANUM\", \"APRO\", \"COM\",\n",
    "    \"CONJ\", \"INTJ\", \"NUM\", \"PART\", \"PR\", \"S\", \"SPRO\", \"V\"]\n",
    "def mystem_parse(texts):\n",
    "    '''\n",
    "        Прогнать список текстов через mystem, вернув для каждого слова\n",
    "        его нормальную форму и граммему.\n",
    "        Аргументы:\n",
    "            texts - список строковых значений;\n",
    "        Возвращает: список наборов, где каждый набор соответствует\n",
    "        входному тексту и содержит кортежи (нормальная форма, граммема)\n",
    "    '''\n",
    "    result = []\n",
    "    # В качестве разделителя сообщений используем волшебное слово.\n",
    "    # Грязно, но работает:\n",
    "    text = \"\\nDEADBEEF\\n\".join(texts)\n",
    "\n",
    "    print(\"Calling mystem...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "    pipe = Popen([\"mystem\", \"-lni\"], stdout=PIPE, stdin=PIPE)\n",
    "    raw = pipe.communicate(text.encode(\"utf-8\"))[0].decode(\"utf-8\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "    msg = []\n",
    "    for line in raw.split():\n",
    "        if \"DEADBEEF\" in line:\n",
    "            result.append(msg)\n",
    "            msg = []\n",
    "            continue\n",
    "        if line[-1] == '?':\n",
    "            # Если mystem не опознал слово:\n",
    "            norm = line\n",
    "            gramm = \"NAME\"\n",
    "        else:\n",
    "            norm = line[:line.find(\"=\")]\n",
    "            gramm = None\n",
    "            line = line.split(\"|\")[0][line.find(\"=\")+1:]\n",
    "            for trait in (\"гео\", \"имя\", \"фам\", \"отч\"):\n",
    "                if trait in line:\n",
    "                    gramm = \"NAME\"\n",
    "                    break\n",
    "            if gramm is None:\n",
    "                gramm = line.split('=')[0].split(',')[0]\n",
    "        assert gramm in MYSTEM_GRAMMEMS\n",
    "        norm = norm.strip('?')\n",
    "        msg.append((norm, gramm))\n",
    "    result.append(msg)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling mystem... Done.\n"
     ]
    }
   ],
   "source": [
    "texts = [row[\"title\"] + \". \" + row[\"text\"] for ind, row in data.iterrows()]\n",
    "parsed = mystem_parse(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число сообщений: 32317\n"
     ]
    }
   ],
   "source": [
    "messages = {}\n",
    "base_date = datetime.datetime.strptime(min(data[\"date\"]), \"%Y-%m-%d\").date()\n",
    "for stem, (ind, row) in zip(parsed, data.iterrows()):\n",
    "    date = datetime.datetime.strptime(row[\"date\"], \"%Y-%m-%d\").date()\n",
    "    stamp = (date - base_date).days\n",
    "    messages[ind] = {\"stem\": stem, \"date\": stamp, \"publisher\": row[\"publisher\"], \"cluster\": row[\"cluster\"]}\n",
    "print(\"Число сообщений:\", len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составляем список пар для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальный кластер: 4381\n"
     ]
    }
   ],
   "source": [
    "intersections = {}\n",
    "allowed_grammems = set([\"NAME\", \"A\", \"ADJ\", \"V\"])\n",
    "for id, msg in messages.items():\n",
    "    for st in msg[\"stem\"]:\n",
    "        if st[1] not in allowed_grammems or len(st[0]) < 4: continue\n",
    "        if st not in intersections:\n",
    "            intersections[st] = set()\n",
    "        intersections[st].add(id)\n",
    "print(\"Максимальный кластер:\", max(len(val) for key, val in intersections.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальный кластер: 2995\n",
      "Число кластеров: 50378\n",
      "Кластеры > 500: [503, 505, 508, 513, 513, 525, 529, 535, 541, 543, 546, 550, 557, 557, 561, 564, 565, 572, 579, 586, 587, 593, 596, 597, 601, 606, 611, 617, 627, 632, 637, 639, 642, 644, 655, 749, 783, 818, 829, 830, 843, 877, 924, 957, 983, 1010, 1021, 1046, 1091, 1133, 1138, 1499, 1583, 1611, 1736, 1737, 1762, 1831, 1935, 2299, 2402, 2826, 2858, 2869, 2887, 2951, 2995]\n"
     ]
    }
   ],
   "source": [
    "day_intersections = {}\n",
    "for st, int_list in intersections.items():\n",
    "    for msg_id in int_list:\n",
    "        msg = messages[msg_id]\n",
    "        for day in range(msg[\"date\"], msg[\"date\"]+2):\n",
    "            key = (st, day)\n",
    "            if key not in day_intersections:\n",
    "                day_intersections[key] = set()\n",
    "            day_intersections[key].add(msg_id)\n",
    "\n",
    "day_intersections = {key: value for key, value in day_intersections.items() if len(value) > 1}\n",
    "ln, key = max((len(val), key) for key, val in day_intersections.items())\n",
    "print(\"Максимальный кластер:\", ln)\n",
    "print(\"Число кластеров:\", len(day_intersections))\n",
    "print(\"Кластеры > 500:\", sorted(len(cluster) for key, cluster in day_intersections.items() if len(cluster) > 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимум пар для классификации: 76534703\n"
     ]
    }
   ],
   "source": [
    "pairs = 0\n",
    "for cluster in day_intersections.values():\n",
    "    #if len(cluster) > 100: continue\n",
    "    pairs += len(cluster) * (len(cluster)-1)//2\n",
    "print(\"Максимум пар для классификации:\", pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число пар семантических дубликатов: 24241\n"
     ]
    }
   ],
   "source": [
    "pairs = set()\n",
    "for cluster in day_intersections.values():\n",
    "    for id1, id2 in itertools.combinations(cluster, 2):\n",
    "        if messages[id1][\"cluster\"] == messages[id2][\"cluster\"]:\n",
    "            pairs.add(frozenset((id1, id2)))\n",
    "print(\"Число пар семантических дубликатов:\", len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1336, (('россия', 'NAME'), 370), 2951), (1305, (('россия', 'NAME'), 371), 2887), (1211, (('украина', 'NAME'), 371), 2402), (1162, (('украина', 'NAME'), 370), 2299), (797, (('новый', 'A'), 370), 1935), (609, (('сообщать', 'V'), 371), 2995), (582, (('сообщать', 'V'), 370), 2869), (579, (('новый', 'A'), 371), 1499), (574, (('российский', 'A'), 371), 1762), (568, (('быть', 'V'), 370), 2858), (561, (('российский', 'A'), 370), 1737), (560, (('быть', 'V'), 371), 2826), (427, (('заявлять', 'V'), 371), 1831), (425, (('москва', 'NAME'), 371), 830), (419, (('москва', 'NAME'), 370), 818), (399, (('заявлять', 'V'), 370), 1736), (385, (('украинский', 'NAME'), 371), 1091), (368, (('украинский', 'NAME'), 370), 1046), (322, (('главный', 'A'), 371), 1010)]\n"
     ]
    }
   ],
   "source": [
    "key_dups = {}\n",
    "for key, cluster in day_intersections.items():\n",
    "    for id1, id2 in itertools.combinations(cluster, 2):\n",
    "        if messages[id1][\"cluster\"] == messages[id2][\"cluster\"]:\n",
    "            key_dups[key] = key_dups.get(key, 0) + 1\n",
    "print(sorted((val, key, len(day_intersections[key])) for key, val in key_dups.items())[-1:-20:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание таблиц признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages_sets = {key: set(msg[\"stem\"]) for key, msg in messages.items()}\n",
    "\n",
    "def make_feature_table(cluster):\n",
    "    rows = []\n",
    "    for id1, id2 in itertools.combinations(cluster, 2):\n",
    "        if messages[id1][\"publisher\"] == messages[id2][\"publisher\"]:\n",
    "            continue\n",
    "        common = get_common(messages_sets[id1], messages_sets[id2])\n",
    "        rows.append( tuple(sorted((id1, id2)))\n",
    "                    + tuple(common.get(col, 0) for col in MYSTEM_GRAMMEMS)\n",
    "                    + (int(messages[id1][\"cluster\"] == messages[id2][\"cluster\"]),)\n",
    "                   )\n",
    "    return ps.DataFrame(rows, columns=[\"id1\", \"id2\"]+MYSTEM_GRAMMEMS+[\"is_dup\"])\n",
    "\n",
    "def get_common(m1, m2):\n",
    "    common = {}\n",
    "    for norm, grammem in (m1 & m2):\n",
    "        common[grammem] = common.get(grammem, 0) + 1\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово.            \n"
     ]
    }
   ],
   "source": [
    "tables = []\n",
    "total = sum(1 for cluster in day_intersections.values() if len(cluster) <= 25)\n",
    "cur = 0\n",
    "for cluster in day_intersections.values():\n",
    "    if len(cluster) > 25: continue\n",
    "    tables.append(make_feature_table(cluster))\n",
    "    cur += 1\n",
    "print(\"Готово.            \")\n",
    "df = ps.concat(tables)\n",
    "df.to_csv(\"data/table-cluster-le25.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73520\n",
      "17685\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop_duplicates()\n",
    "print(len(df[df[\"is_dup\"] == 1]))\n",
    "print(len(df2[df2[\"is_dup\"] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv(\"data/table-cluster-le25.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание таблицы одного кластера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n"
     ]
    }
   ],
   "source": [
    "cls_key = (('москва', 'NAME'), 371)\n",
    "print(len(day_intersections[cls_key]))\n",
    "df = make_feature_table(day_intersections[cls_key])\n",
    "df.to_csv(\"data/table-cluster-москва-371.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
